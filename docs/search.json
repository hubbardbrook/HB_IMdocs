[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Data Management Plan",
    "section": "",
    "text": "1.1 History of IM at the Hubbard Brook Experimental Forest\nThe Hubbard Brook Experimental Forest (HBEF; USDA Forest Service) was established in 1955 and became an NSF-funded Long Term Ecological Research Site (HBR) in 1988. Information management has been an important component at Hubbard Brook from its inception. Data and documents from 1955 onward have been stored and protected, and although most of these early items consist of physical assets (paper charts, photographic slides, field notes, handwritten data, publications, etc), much of this material has been converted to digital format, with original copies in fireproof storage at the HBEF Pierce Lab and at the Northern Research Station in Durham, NH.\nThe establishment of the LTER-HBR occurred at a time of rapidly changing technology; desktop computers and email were new, and the internet as we know it was still several years away. The Hubbard Brook community fully embraced these emerging technological resources, and established access to data with the ‘Source of the Brook’, a public access dial-up electronic bulletin board, which allowed easy retrieval of many data sets from the HBR (1990). From a dialup bulletin board and gopher server in the early 1990s, to the World Wide Web in the late 1990s, HBR’s latest technology advances in publicly sharing data and resources has seen a migration of the website to WordPress, a data catalog built on dynamic access to content in the Environmental Data Initiative repository (EDI), bibliography management in Zotero, to name a few…\nUntil 2012, Information management for HBR was provided through the Forest Service, with John Campbell filling this role from 1997-2012. During this time, the LTER network adopted EML (Ecological Metadata Language) as a metadata standard, and HBR was an early adopter of this standard. In 2003-4 the first EML-based data packages were prepared for HBR with online download access and formatted browser display of metadata.\nFunding for the HBR Information Management position was provided in the 2010 renewal of LTER-HBR funding (HBR5), and the position was filled in mid-2012 by Mary Martin (Earth Systems Research Center, University of New Hampshire, Durham, NH).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#governance",
    "href": "intro.html#governance",
    "title": "1  Data Management Plan",
    "section": "1.2 Governance",
    "text": "1.2 Governance\nInformation management at the Hubbard Brook Ecosystem Study (HBES) is guided by the Information Oversight Committee (IOC), which meets on an ad hoc basis with virtual IOC meetings scheduled accordingly.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#research-approval-committee",
    "href": "intro.html#research-approval-committee",
    "title": "1  Data Management Plan",
    "section": "1.3 Research Approval Committee",
    "text": "1.3 Research Approval Committee\nA Research Approval Committee (RAC) has been established to assist the Forest Service and broader HBES community in making decisions regarding what research studies will be allowed. In making its recommendation, the RAC considers a number of factors related to: (1) the relationship of the proposed project to the overall Hubbard Brook Ecosystem Study (how does this project fit into the overall study; why is it important for this research to occur at the Hubbard Brook Experimental Forest, as opposed to some other site); (2) the scientific merit of the proposed research; (3) the integrity of the site (e.g. how will this research impact the Forest or other ongoing research projects); and (4) the extent to which the proposed research compromises or enhances ongoing efforts. The RAC’s critical review of proposed research at Hubbard Brook helps ensure that the scientific value of the Hubbard Brook Experimental Forest is maintained for the future.\nProposal submissions to the RAC are made through JotForm webforms on https://hubbardbrook.org/research/research-proposal-submission, and are currently being used to develop a project-level database to support the RAC review process and IM tracking of data collections at Hubbard Brook. Researchers approved by the RAC are encouraged to submit data to be included in the Hubbard Brook Data Catalog, regardless of funding source.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#overview",
    "href": "intro.html#overview",
    "title": "1  Data Management Plan",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nHBR data packages are prepared for submission to the Environmental Data Initiative (EDI), following best practices developed over 4 decades by the LTER Information Management community (https://ediorg.github.io/data-package-best-practices/eml-best-practices.html). These best practices, and the efforts of the EDI, ensure that data are Findable, Accessible, Interoperable, and Reusable, following the principles of the FAIR initiative. The EDI serves as the primary repository for HBR data, and details about the operation of EDI can be found at https://edirepository.org.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#data-holdings",
    "href": "intro.html#data-holdings",
    "title": "1  Data Management Plan",
    "section": "3.2 Data Holdings",
    "text": "3.2 Data Holdings\nThe HBR data catalog has had a strong emphasis on long-term datasets and data from the major watershed experiments. Many of these data pre-date the establishment of LTER-HBR, and are now available as a result of a 60+ year culture of robust data curation and sharing. More than 20 HBR data packages have been collected over a period of 50 or more years, with another 30 covering a timespan of more than 20 years. Through close coordination with the Research Approval Committee, the Hubbard Brook Committee of Scientists, and project administration, HBR-IM is able to identify datasets that can be incorporated into our data catalog. Graduate students working at Hubbard Brook are also surveyed periodically to identify forthcoming datasets and they are also trained in the EDI data publication workflow.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#metadata-standards",
    "href": "intro.html#metadata-standards",
    "title": "1  Data Management Plan",
    "section": "3.3 Metadata Standards",
    "text": "3.3 Metadata Standards\nAll HBR data packages are prepared for submission through the development of metadata in the Ecological Metadata Language standard (EML; https://eml.ecoinformatics.org/eml-ecological-metadata-language). Basic EML content includes: title, abstract, personnel, contacts, publication date, spatial and temporal coverages, keywords (consistent with LTER controlled vocabulary), project funding, publisher, data access and use policies, and detailed attribute-level metadata. Data download and use is facilitated through the fully described attribute metadata (column names, definitions, units, missing values, and coding). The highest level of EML completion is achieved through the EDI congruency checker with informational, warning, and error messages that provide feedback to HBR-IM on additional steps that can (or must) be taken to submit to the repository. These congruency checks read both metadata and data, testing a minimum of 40 conditions that can be addressed to insure that data packages are are fully capable of integration with other data, and fully operational in higher level workflows and automated data processing.\nIn 2019, LTER sites began using EML2.2. EML2.2 provides the structure to accommodate a number of advanced metadata elements. Of note, is the ability to annotate data packages at the data package, entity, and attribute level, by linking to persistent identifiers in ontologies, such as those found at https://bioportal.bioontology.org/ontologies and elsewhere. As the annotation elements in EML2.2 become populated, both the discoverability of datasets, and the ability to use datasets in synthesis efforts will be enhanced. HBR-IM has been a member of both the EDI Semantics Working Group (formed in January 2019), and the EDI/LTER Units Working group (2023-present), which have a goal of developing best practices and training on the use of the new EML2.2 annotation elements. The Units Working Group is responsive to recommendations from the LTER 40-year review on facilitating synthesis efforts. This has been accomplished by establishing a relationship with the QUDT ontology (https://qudt.org), developing code to map ad hoc units to this ontology, and preparing a manuscript on this effort for publication.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#data-package-development",
    "href": "intro.html#data-package-development",
    "title": "1  Data Management Plan",
    "section": "3.4 Data Package Development",
    "text": "3.4 Data Package Development\nHBR IM has fully adopted the EDI ezEML application for data package development. This cloud-based application was developed by EDI and users are supported by the EDI team and ezEML developer. This application continues to be developed to support emerging LTER/EDI data requirements. Within this system, HBR can store templates for access and re-use of common metadata elements (people, taxonomy, geographic coverage, funding, etc). EzEML also supports a ‘collaboration’ mode, where IM and data creators can work together to complete the full metadata documentation necessary for the repository.\nA separate chapter describes the data package development workflow in detail: HBR Data Package Development.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#data-quality-control",
    "href": "intro.html#data-quality-control",
    "title": "1  Data Management Plan",
    "section": "3.5 Data Quality Control",
    "text": "3.5 Data Quality Control\nThe HBR research community is widely dispersed among different institutions and laboratories, and data quality control is implemented primarily by the individual researcher. All data packages include methods, wherein detailed data QC protocols can be documented. The HBR-IM works with research teams to document quality control in the data package metadata as appropriate. This may range from descriptions directly in EML, PDF files uploaded with data packages, or cross referencing to details on data QC available elsewhere. IM provides the data submitter with feedback on a number of QC checks that are implemented during the data package development workflow. These include value ranges for data table attributes, coding consistency, and additional issues that are flagged within the ezEML environment and the EDI congruency checker (the final checks during repository upload).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#access-policy",
    "href": "intro.html#access-policy",
    "title": "1  Data Management Plan",
    "section": "3.6 Access Policy",
    "text": "3.6 Access Policy\nThe HBR data policy follows that of the LTER Data Access Policy, as updated in 2017. (https://lternet.edu/data-access-policy/; Creative Commons license - Attribution - CC BY; https://creativecommons.org/licenses/by/4.0/). All pre-existing HBR data packages have been revised to include this new policy, and the policy is linked on the hubbardbrook.org information management page. The policy reads as follows:\n\nThis information is released under the Creative Commons license - Attribution - CC BY (https://creativecommons.org/licenses/by/4.0/). The consumer of these data (“Data User” herein) is required to cite it appropriately in any publication that results from its use. The Data User should realize that these data may be actively used by others for ongoing research and that coordination may be necessary to prevent duplicate publication. The Data User is urged to contact the authors of these data if any questions about methodology or results occur. Where appropriate, the Data User is encouraged to consider collaboration or co-authorship with the authors. The Data User should realize that misinterpretation of data may occur if used out of context of the original study.\nWhile substantial efforts are made to ensure the accuracy of data and associated documentation, complete accuracy of data sets cannot be guaranteed. All data are made available “as is.” The Data User should be aware, however, that data are updated periodically and it is the responsibility of the Data User to check for new versions of the data. The data authors and the repository where these data were obtained shall not be liable for damages resulting from any use or misinterpretation of the data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#data-access",
    "href": "intro.html#data-access",
    "title": "1  Data Management Plan",
    "section": "3.7 Data Access",
    "text": "3.7 Data Access\nThe complete inventory of Hubbard Brook data can be browsed, filtered, and searched on the HBR website https://hubbardbrook.org/d/hubbard-brook-data-catalog. Data are also discoverable through both the EDI data portal (https://portal.edirepository.org), through DataONE (https://search.dataone.org), google dataset search (https://datasetsearch.research.google.com/), and through DataCite (https://datacite.org), the entity providing dataset DOIs to EDI. A separate stand-alone document is generated as needed (for proposals and reviews), and describes all HBR data packages in the EDI (and other) repository – Hubbard Brook Data Catalog Inventory.\nAlso see ESRC Computer Resources appendix 1.\nTable 1 outlines software in use by HBR-IM to manage data package development and the Hubbard Brook website.\nTable 1. Features of HBR Information Management System\n\n\n\nFeature\nDetails, software, resources\n\n\n\n\nWebsite: https://hubbardbrook.org\nWordPress, html, css, php, xslt, javascript, apache, piwigo\n\n\nBibliography\nZotero, Zotpress wordpress plugin\n\n\nData Catalog\nEDI data repository, local WordPress gateway to EDI HBR data, EDIutils R\n\n\nMetadata\nezEML, PostgreSQL, EML R package, EML2.1\n\n\nComputer Hardware\nDell Poweredge R510, desktop and laptop linux systems.\n\n\nBackup\nBackupPC, rsnapshot, daily, weekly and monthly backups, on and off-site\n\n\nData management\nR, LibreOffice, QGIS, MySQL, PostgreSQL, git",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#samples",
    "href": "intro.html#samples",
    "title": "1  Data Management Plan",
    "section": "5.1 Samples",
    "text": "5.1 Samples\nIn 1990, an archive facility was built at the Hubbard Brook Experimental Forest to store samples permanently so that they will be available for future research. The 1860 sq. ft. building consists of two rooms: a larger unheated room (30 x 46 ft.) and a smaller room (16 x 30 ft.) heated to just above freezing in the winter. The larger room is uninsulated and is subject to large variations in temperature and humidity; the most scientifically valuable samples are stored in the smaller, insulated, heated room.\nThe archive building now houses approximately 70,000 samples of soil, water, plant tissue, and other materials. Samples are preserved, barcoded, and cataloged with accompanying metadata in a database. This database of bar-coded samples is searchable online at https://data.hubbardbrook.org/samples/. In 2024, both the underlying collection and sample database and the search interface are being restructured. This effort is resulting in improved collection organization and more detailed sample-level metadata.\nRequests for reanalysis of archived samples (e.g. isotopic analyses, heavy metals, etc.) are received periodically, and have resulted in a number of publications.\nA Sample Archive Committee (SAC) was formed in 2013 to address storage space, future direction, priority for continued bar-coding, etc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#sample-archive-subsampling-policy",
    "href": "intro.html#sample-archive-subsampling-policy",
    "title": "1  Data Management Plan",
    "section": "5.2 Sample Archive Subsampling Policy",
    "text": "5.2 Sample Archive Subsampling Policy\nHBR shares these archived samples with scientists upon request. As stewards of these samples, our highest priorities are:\n\nto maintain the chemical integrity of these samples;\nto document the use of these samples, and any resulting changes;\ninform principal investigators of interest in using them;\nto acknowledge the appropriate funding sources for their original collection.\n\nDetails on the subsampling of archived material can be found on the sample request form:\nhttps://hubbardbrook.org/sites/default/files/documents/subsampling_request.pdf",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "intro.html#directions-for-sample-submission",
    "href": "intro.html#directions-for-sample-submission",
    "title": "1  Data Management Plan",
    "section": "5.3 Directions for Sample Submission",
    "text": "5.3 Directions for Sample Submission\nRequirements for acceptance of samples into the archive:\n\nAdequate documentation must accompany physical samples.\nSamples are stored in either an unheated large room or a smaller room that is heated to just above freezing. The contributing scientist is responsible for deciding that these conditions are suitable for his/her samples.\nSoil samples must be air or oven-dried and stored in plastic or glass bottles with screw caps to ensure a tight seal. Cardboard is not permitted.\nVegetation samples should be dried, ground and stored in clear plastic or glass containers.\nWater samples must be stored in plastic bottles and will be accepted either treated, or not. If treated, the investigator must specify the type and concentration used.\nAll tree logs, cookies and cores should be air-dried and can be stored in cardboard boxes or arranged in a manner that will allow for air to flow between individual samples. Tree cores should be mounted or stored in straws.\nSamples that are considered toxic may be rejected. The data management committee may confer with the SAC about important, but toxic samples requiring storage.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "DataPackageWorkflow.html",
    "href": "DataPackageWorkflow.html",
    "title": "2  Data Package Workflow",
    "section": "",
    "text": "2.1 Overview\nThe purpose of this document is to capture details of the data package development workflow that is currently in use at Hubbard Brook. HBR data is published in the Environmental Data Initiative Repository (EDI). With the availability of EDI’s ezEML data package builder application (adopted by HBR in 2024), this once long and complicated process has been greatly simplified.\nIn 2024, HBR fully adopted the EDI ezEML workflow for data package development. All data packages are developed under the EDI HBR user account. The division of effort varies with the nature of the data package. Graduate students are encouraged to collaborate with the IM online within the ezEML environment. This serves as a way to directly input metadata without first populating a template, reduces IM time on some data packages, and is an important skill-builder for HBR graduate students.\nEzEML provides the capability to store often-used metadata components in a template. For HBR, there is one master template and additional templates for HB projects that are frequent publishers (MELNHE, HBWaTER, BIRD, CRCH). Since the master template is very extensive, it is not cloned as a starting point for a new dataset (which might be a common template use), but instead accessed through the import [creator, geographic, keywords, project, funding] buttons where just selected items are brought into the current dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Package Workflow</span>"
    ]
  },
  {
    "objectID": "DataPackageWorkflow.html#data-package-development-workspace",
    "href": "DataPackageWorkflow.html#data-package-development-workspace",
    "title": "2  Data Package Workflow",
    "section": "2.2 Data package development workspace",
    "text": "2.2 Data package development workspace\nThe working directory for package development is on the HBR-IM desktop with the home directory for data package managment identified elsewhere as $DPM_HOME.\nAssets for each data package are in folders named $DPM_HOME/ezEML/hbr[pkgid]. While most of the workflow occurs in the ezEML environment, this local filesytem is used to handle dataset assets submitted to IM (metadata templates, datafiles, etc). The completed ezEML packages are downloaded (as zip) to this location for subsequent upload to the EDI staging and production servers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Package Workflow</span>"
    ]
  },
  {
    "objectID": "DataPackageWorkflow.html#step-by-step-data-package-workflow",
    "href": "DataPackageWorkflow.html#step-by-step-data-package-workflow",
    "title": "2  Data Package Workflow",
    "section": "2.3 Step-by-Step Data Package Workflow",
    "text": "2.3 Step-by-Step Data Package Workflow\nThe HBR Data Inventory table is hosted on the HubbardBrook sharepoint site (HBR-IM administrator at UNH). This table contains additional information that we use on our local data catalog to enhance user experience (flagging of significant core datasets, more robust LTER Core Research Area assignments that may be missing in older metadata, and a code to categorize datasets and to sort them in the initial catalog view). Data packages are entered in this table as soon as they are identified (in some cases with very long lead times). Upon becoming aware of a dataset, a package id is assigned and the entry initiated with status=anticipated. As soon as data and/or metadata are in-hand, the status is updated to ‘draft’. The table includes packageID, abbreviated title, contact, notes as needed, flagging as long-term core dataset, and EDI submission status.\nThe steps are as follows:\n\nComponents for data packages are provided to the IM through a sharepoint dropoff.\nezEML collaboration is established if desired for the dataset.\nA data package is initiated in ezEML with the naming convention of hbr[pkgid]-[shortname].\nThe HBR Master Template (stored in EDI) is used to import people, geographic areas, keywords, projects, and funding.\nData tables are loaded from csv.\nData table attributes are documented either directly on the online forms or through the ezEML table entity templating feature (a great timesaver for complex datatables).\nezEML data package is downloaded to IM’s computer\nThe R script to add QUDT unit annotations is run\nThe annotated eml file is uploaded to portal-s.lternet.edu and the URL is shared with creator for review\nSubsequent edits are made with creator feedback\nData package is approved by the creator\nData package is uploaded to the live repository",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Package Workflow</span>"
    ]
  },
  {
    "objectID": "DataPackageWorkflow.html#non-tabular-datasets-images-audio-very-large-datasets-etc",
    "href": "DataPackageWorkflow.html#non-tabular-datasets-images-audio-very-large-datasets-etc",
    "title": "2  Data Package Workflow",
    "section": "2.4 Non-tabular datasets (images, audio, very large datasets, etc)",
    "text": "2.4 Non-tabular datasets (images, audio, very large datasets, etc)\nHubbard Brook has published a number of datasets that contain zip files of pdfs, images, audio files, etc. Guidance for preparing these special case datasets can be found in the EML Best Practices document.\n\n2.4.1 Large Datasets\nIn some of these cases, the data entities are quite large and cannot be uploaded with the browser interface (500Mb max), but are within the size cap for online EDI data storage. At the current time, large datasets are staged on a UNH server with the distribution URL set to that location. In some cases we develop packages using a smaller placeholder file so that we do not overload storage on the ezEML platform or portal-s staging area. Datasets exceeding the 100Gb threshold are deemed “too large for HTTP” and must be prepared as offline data entities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Package Workflow</span>"
    ]
  },
  {
    "objectID": "DataPackageWorkflow.html#notes-on-revising-older-datasets.",
    "href": "DataPackageWorkflow.html#notes-on-revising-older-datasets.",
    "title": "2  Data Package Workflow",
    "section": "2.5 Notes on revising older datasets.",
    "text": "2.5 Notes on revising older datasets.\nWhen earlier data packages are revised, the starting point is an ezEML fetch of the published data package. Steps are similar to those used for a new dataset, but remember to increment the revision number. Assets for earlier data packages developed can be found in either the ‘EMLassemblyline’ or ‘legacy’ folders, although those files should rarely be necessary once a data package is published in EDI.\nAn EDI fetched dataset may have been developed with a non-ezEML workflow. If that is the case, items requiring attention will be:\n\nCreators – delete and import from the template to be sure to get ORCIDs and institution RORs for each person. Use the ‘sort’ function on people import to find them easier in the long list.\nProject – EMLAL datasets may have funding in ‘related funding’ or a text string in project abstract. Delete these. Import all funding from the template as primary or related. The template will have enhanced information to include grant url, funding agency ROR, etc.\nIntellectual rights will be correct for all older datasets, but it is best to reset that in ezEML to CC-BY selection.\nIncrement the packageId revision number.\nUse re-upload datatable if revision includes new or modified data. This all goes well if the table is identical. If there are new columns, upload as a new table and clone metadata from the original, then define any new columns. If the dataset was prepared in EMLAL, clear min/max bounds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Package Workflow</span>"
    ]
  }
]